Data Science is the field which extracts data as to process and visualise it, as to communicate a message. It is done through the use of [[Machine Learning|machine learning]] techniques and other forms of statistics as to create predictions given past data. Data science follows the process:
1. **Collection**: getting the data.
2. **Engineering** storage of the data.
3. **Governance**: management of the privacy and standardisation of the set.
4. **Wrangling**: data preprocessing and cleaning.
5. **Analysis**: analysing data as to make discoveries.
6. **Presentation**: expression of how the data is impactful.
7. **Operationalism**: putting the results to work.

Within this chain, a data scientist focuses on 1, 4-6, while knowing about all the other steps. With a chief data scientist managing the whole process.

# Data Science Roles
Within Data science there are a few main roles. These are:
- **Data Analyst**, are primarily people who use data to find trends.
- **Data Scientists** develop products or ideas with data, following the process from collection to analysis, and application.
- **Data Engineers** are people who manage the data infrastructure, automating data processes and deploying models.

All roles within data science are expected to have skills in business, ML, mathematics, programming, and statistics.

# Data Visualisations
A data visualisation is the portrayal of data through the use of graphing techniques. These enable easy viewing of patterns found within the data. Some common visualisations are:
- **Bar Charts** which portrays one discrete set of data, and one continuous variable.
	- **Histograms** are a form of bar chart that measures a continuous set through applying discrete subsets of the data. This widths are defined by the minimum and maximum divided by the amount of bins.
- **Frequency Table** which functions similar to a histogram but in table form.
- **Pie Charts** which are used to measure how multiple categories make up a percentage.
- **Motion Charts** are an interactive chart that portrays multi-dimensional data. These can only be used for non-static uses.
- **Box Plot** measures outliers, whiskers, and the 1st/3rd quartile.

# Data Collection
Data sets are the resource which data science runs on. There are two main common resources within data sets, these are open, and private data sets. An **open data set** is sourced from a government or IT organisation, and shared for no cost. Many of these data sets are available through an Application Program Interface, or **API** which is a large set of data that is publicly available through a system. The data is pulled from a central data set. This allows the creators to update the data set whilst others use the data.

**Big Data** is the result of datasets pushing the constraints of a system's capability. This is said to be the result of the **volume** of the data, the **velocity** of the collection, the **variety** of data forms, and **veracity** in the uncertainty of data.

## Metadata
Metadata is the structured information that describes underlying details about data. Metadata can be considered:
- **Descriptive**, with it describing the content for identification.
- **Structural** which documents relationship and links in the data.
- **Administrative** that helps to manage information.

# Data Wrangling
Data wrangling is the process of conforming a given data set to the constraints of the problem. This process involves the pre-processing, preparation, cleansing and transformation to create a coherent data set. The main problems with data that can be solved with wrangling are:
- **Interpretability issues** are when data lacks obvious labels for each column or row. This leads to the data being hard to interpret.
- **Data format issues** are issues which are caused from a lack of a correct format which makes segments of data incompatible.
- **Inconsistency issues** are caused by human error in mistyped inconsistent or even irrelevant entries of data.
- **Missing values** are absent segments of data that need to be interpreted.
- **Outliers** are observations which stray from the norm in ways that could harm the model.
- **Duplicates** are segments of data which have been repeated.

## Data Processing
Big data can be processed in a variety of ways, the most common being [[Bash]], [[Databases]]. Databases commonly use a SQL database for structured data, and NoSQL for changing data with no structure.

Processing data can be done through a variety of methods. These are:
- **Batch** which stores segments of large data in blocks.
- **Streaming** which feeds data into the system.
- **Interactive** which allows for individuals to determine data used.

**Map-Reduce** is a framework which splits the data, maps it to values, and reduces the results. This allows for parallel computing techniques to be used. **Hadoop** is an open-source Java implementation of this. **Spark** builds upon Hadoop and enables streaming capabilities.

# Data Governance
Data governance is the process of managing data in a way that protects, controls and enhances it. To organise data a **data management plan** is useful. Privacy is a topic within this plan that has to adhere to the regulations of the government. If the plan follows the regulations the plan is considered to have compliance. Audits are a validation of compliance.

Data can be considered **implicit** if the information can be inferred from the regular data.
