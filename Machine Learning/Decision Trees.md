Decision trees are a [[Machine Learning|machine learning]] algorithm used for [[Classification|classification]] problems. A decision tree consists of multiple tree nodes which all function as a classifier that leads to the leaf nodes. This is done through recursive partitioning the feature space, and grouping similar instances together. On a graph this would resemble sections on a graph. Decision trees are good as they allow a way to handle non-linear relationships that contain continuous, categorical and count predictor variables. Despite these advantages decision trees suffer from the fact they are sensitive and inefficient to compute.

A decision tree works by taking the predictor space and subdividing it up into $L$ disjoint regions. This allows the tree to build separate simple models. For **regression trees** where the targets are continuous a [[Probability Distribution Models#Gaussian (Normal) Distribution|normal distribution]] with a different mean $\mu_i$ for each leaf $R_i$ is used. For binary classification a Bernoulli distribution with different probabilities of success is used. 

Training decision trees usually involves using a model selection approach on a tree as like [[Regression Analysis#Information Criteria|information criteria]] or [[Regression Analysis#Cross-Validation|cross-validation scores]] to find a tree with a good fit. The most basic algorithm to find a decision tree works by starting with a leaf node, trying to split on every predictor and compute criterion score, if none of the splits improve the tree stop, otherwise choose the split that results in the tree with the smallest score, and repeat the split. Splits are usually found through computing the log-likelihood.

Training decision trees can be trained with cross validation by pruning the true through using the cross validation score. This is done by partitioning data into equal sized subsets, and growing the tree using all partitions, and then pruning the tree back. This repeats until all leaves are found.

# Random Forests
**Random Forest** is an ensemble learning method that constructs multiple decision trees for each variable. This decreases the sensitivity of the decision tree, and decreases the computational complexity of the problem.
