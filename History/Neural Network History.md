In many ways the history of neural networks is defined by an exploration of the human brain through recent developments in neurology and computing. However, as time has past neural networks have evolved to explore not only the human mind, but every axis of life. As to understand the current movements within deep learning, it is first important to discover the roots that established this worldwide transition of neural networks.

# McCulloch & Pitt Neurons
Neural Networks starts their conceptualisation in Warren S. McCulloch and Walter Pitts' paper *A Logical Calculus of the Ideas Immanent in Nervous Activity*. This paper is ambitious in its approach, evident through the conclusion which talks of the "ignorance, implicit in all our brains", as a result of the "fact the \[personal neural\] net \[can\] be altered". However, despite the extravagance of their language McCulloch and Pitt establish many early axioms which would be used throughout later neural networks. This is seen through their conclusion that a net is Turing complete. An idea established through rigorous theorems provided throughout the document. Beyond these theorems and mathematical notation (that will be discarded by modern scholars anyway) which make up the rhetoric of the paper there is the base assumptions which contextualise the paper and many neural network endeavours. Those being:
- Neurons keep a level of excitement that is considered "all-or-none".
- A certain number of synapses must cause the excitation of a neuron.
- These synapses can be inhibitory.
- There is no significant time delay within neurons.
- The structure of a net doesn't change with time.

Now while many of these facts prove false for a brain some of these ideas are seen within modern neural networks. Modern neural networks remain constant in their synaptic connections, with weights of 0 being equal to those connections that don't exist (This proved within a theorem =). While not being "all-or-none" neurons still hold a state that is determined by the the summation of all connected synapses. Furthermore, the evidence of symmetry can be a far off call to back propagation techniques that would be later seen. And finally the assertion that a functor exists for all states is similar to that of an activation function. From this information one may derive the basic $\sum {} {} {x;w;}$ from this paper.

While McCulloch and Pitt's research can, and is, seen as foundational in many neural network concepts. It still remains a far abstraction from even the perceptron's of the 1950s and 60s. This is resultant of the unfocused direction of the paper, as well as the lack of seemingly practical applications.

# Hebbian Learning
Hebbian learning in many ways characterised many of the future neural network developments through a further theorisation of neural activity. First expressed in first in Donald O. Hebb's *The Organization of Behavior: A Neuropsychological Theory*, Hebb's contribution saw the cumulation of neuroscience of the time, that including developments made by McCulloch and Pitt. This cumulation led to a postulate that learning happens through the changing of synaptic strengths or neural weights. This biological methodology further saw Hebb theorises synaptic connections as symmetric and the product of these being the learning rate of a neuron. However, out of all axioms Cell Assembly Theory proves to be one of the most influential as it states the concept that as learning occurs the strength between synapses change as to promote efficiency. This process can not only strengthen but also create new synapses if the neurons fire together. An idea which would be applied through the changing of weights within modern neural networks. Thus, as a result of these developments Hebbian theory exists as one of the fundamental axioms behind early Neural Networks, as it furthers the rules of neurons established throughout neuroscience to this point. Allowing future developments of these as computational models.

# SNARC
Hebbian principles would first be applied within Marvin Minsky's *Stochastic Neural Analog Reinforcement Calculator* or SNARC for short. This network in many ways seems to be the first neural network developed. The ideas being first theorised in 1949 before practical creation in 1951 after Minsky secured funding from the air force. With 40 Hebb neurons the network used a form of reinforcement learning as to adjust the weights. Thus, the agent would learn to move through a maze. Despite most likely being the first neural network outside of the latter information little is known, as there was no formal paper written about it nor buzz like around later neural network projects. Thus, while in many ways SNARC should have been pivotal in the proliferation of neural networks through its use of Hebbian theory, it remains more a symptom of recent develops rather than the catalyst of future ones.

# Rosenblatt's Perceptron
The conceptualisation of the Perceptron is first introduced through Frank Rosenblatt's 1957 paper *The Perceptron, A Perceiving and Recognizing Automaton*. This paper would then be refined throughout 1958 in his follow up paper named *The Perceptron: A Probabilistic model for information storage and organization of the brain*. Rosenblatt's 1957 paper focuses on the basic architecture of a neural network and its probabilistic principles. This is as opposed to the deterministic machines being developed at the time. Its construction featured three components, those being:
- **Sensory System** which read inputs as to provide the perceptron with a data set.
- **Association System** which receives impulses from the sensory units and transmits outputs characterised by "the algebraic sum of input pulses necessary to exceed" the thresholds.
- **Response System** which used mutually exclusive outputs to provide a result based on "the mean or net value of the signals received".

From these basic concepts Rosenblatt would further relate his model to "Hebb's Cell assembly and Hull's cortical anticipatory goal response" as to postulate that neural networks follow basic ideas established within neuroscience. However, despite these relations Rosenblatt focuses on the computational logic of the system through an expression of probability. The accuracy of the system dependent on a training process which "through exposure to a large sample of stimuli" would convert the randomly associated weights of a network to refined ones. This results in an "approach to the probability of a correct response to a previously reinforced stimulus". A correct response which can be tested through a testing set which is separated from the training one.

The 1958 paper saw many theoretical approaches to the training algorithm of the network. This usually involved probability functions which would relate neurons together. One approach saw a bivalent model which would hold negative weights. Despite initial success Rosenblatt postulates the greater accuracy of a monovalent model, an idea which still in place. While the training algorithms proposed demonstrated great accuracy on some theoretical problems, the lack of an ability to train a across multiple layers would in many ways lead to the future AI winter.

Rosenblatt concludes with his view that through the perceptron "fundamental laws of organization" could be found, not only in relation to computing but the human mind. The seminal words of both these papers and later research published by Rosenblatt would establish the early paradigms of neural networks and transition neural networks from a stage of pre-science to a new field, that would experience further research throughout the next decade.

# ADALINE
So far neural network development has been taken mostly from the perspective of neurologist, and computer scientists, however Bernard Widrow's 1960 technical report named *An Adaptive Adaline Neuron using Chemical Memistors* would offer an engineering perspective on the creation of neural networks. The construction of Widrow's network was largely based upon the memistor. Invented in Widrow's paper *Adaptive Switching Circuits* the memistor was an electronic component that through resistance would be able to keep a weight. Through these memistors Widrow was able to create a neuron that took a set of inputs that would be summed and put through a quantizer. During the training phase these weights would be adjusted by a positive or negative voltage magnitude. The direction determined by the weights which decrease error by $\frac 1 {neurons}$. This would result in a reduction of the error and convergence upon "a stable root-mean-square value". If Rosenblatt's original paper was a shot in the right direction Widrow methodically refined this shot. Not only was Adaline faster than the perceptron but many of its methods improved upon previous developments. In many ways Adaline is understated in its overall importance to the field, as it expanded the ability of neural networks and found many ways to get around the commonly associated limitations.

# Ivakhnenko
While most neural network research was restricted to nations outside of the soviet block, throughout the 1960s and 70s an increase in AI research led to

# AI Winter
The history of AI is commonly understood through a model of periods of rapid growth and falls from grace. This can be seen in the segmentation of early AI and the AI winter over the period of the 70s. However, this abstraction in many ways proves inaccurate, with many key discoveries within adjacent fields leading to later AI development. This also ignores many of the non-USA and UK led initiatives within AI that happened over the period. Even the causes and a correct definition of this period provide inconclusive. From these shortcomings the idea of an AI winter in many ways proves false, however, discarding this idea entirely ignores many of the limitations of AI development during this period. Thus, this section remains as to inform the cause, and totality of this undefined period.

There are many causes to the AI Winter, many which are over valued. One prognosis is the general interest cycle, which sees a popular idea fall to the wayside as others develop. Now while this is true it simplifies the reasons behind this fall. The main reasons being a lack of confidence within the technology leading to a cut in funding which hampered any developments during this period. This is first seen within the Automatic Language Processing Advisor Committee's (ALPAC) report which concluded that machine translations, a process commonly done through AI, should be considered a too difficult for computers to compute. This led to a cut in funding and investigation into other funding objectives which were lacking results. The next commonly attributed causal factor is the proliferation of the Von Neumann architecture, which many saw as opposing the neural network methods of computation. Thus, through the inability of neural networks to function well on Von Neumann hardware many turned away. One factor which is considered significant by many as the cause of the AI winter is Minsky and Seymour Papert's book *Perceptron: An Introduction to Computational Geometry*. In many ways this book is highly educational in its discussion the perceptron, however throughout the book its also explores the limitations of neural networks. The discussion of challenges that face AI was furthered through Minsky's speeches on the topic, leading to a split within the community. Minsky in many ways provided the anomalies which would exist within AI, and particularly neural network research throughout the 1970s and 80s (and still remain to this day). In many ways Minsky is seen as the transitionary figure to the AI winter his responsibility only extends to his influence of others. As it is not through one man a winter makes, but rather a general apathy generated by many. The Lighthill report, can be seen as the final nail in the coffin as its condemnation of AI research, funded by the UK government, led to a severe laps in funds for AI. The Lighthill report concludes that due to its failure to achieve its "grandiose objectives" and inability to solve the intractability and combinatorial explosion seen within many AI problems, AI funding should be redirected to other disciplines. This destruction of AI funding within the UK marks the totality of the AI winter.

The AI winter can be considered a transitional event in AI history. Its impact marks a low point for its history however also illustrates how ideas fall out of scientific favour. However, most significantly the next few sections will show how while AI research wasn't being pursued there were still many developments by adjacent disciplines that would lead to future AI breakthroughs.

# Automatic Differentiation
In 1970 in the field of mathematics Seppo Linnainmaa would publish a paper on automatic differentiation called *Taylor Expansion of the Accumulated Rounding Error*. While no one at the time would realise it this paper would be invaluable to the field of neural networks as it proposed an algorithmic approach to finding the derivative of a function. This process called reverse mode automatic differentiation is a general algorithm for finding derivatives at a point through the segmentation of primitive functions such as addition, exponential, etc, which are recombined with the chain rule. This expression is then evaluated from the outside in finding the Jacobian at any point. While Linnainmaa never alluded to its use within neural networks; automatic differentiation would become the base of backpropagation. As backpropagation is a form of automatic differentiation which produces a scalar results. Despite this paper having the solution to the optimisation of neural networks which led to the AI Winter it would ignored, partially due to its Finnish origin and time taken to translate. While not immediately influential Linnainmaa's paper would establish future methods that would solve the anomalies facing neural networks.

# Neural Network Winter
The next AI winter is in many ways only localised to neural networks. As the research of the 80s gave to the 90s, the popularity of neural networks fell out of favour as new developments within the AI field happened.

# Post Word
Usually this would be a foreword, however due to the note taking style of this document I will leave this as a footnote of sorts. I'm writing this as a critique of what I hope above is a decent piece of writing, which has flaws. These main flaws being the lack of substantiation of many conclusions I make throughout the writing, as well as the lack of primary sources outside of the central papers this is centered around. Now while these flaws may be later amended in hopes of publication for now I the unexperienced historian, and computer scientist will leave these issues in hope a future revision will be made.