# Machine Learning Notes
The general development of Machine learning follows:
- Thomas Bayes *An Essay Towards solving a problem on the doctrine if chances* released - 1763
- Adrien-Marie Legendre creates mean least squares *New Methods for Determination of the Orbits of Comets* - 1805
- Gauss made a similar mean least squares *Theory of the Motion of the Heavenly Bodies Moving About the Sun in Conic Sections*, guess noted have finding this first over Legendre - 1809
- Bayes Theorem furthered by Pierre-Simon Laplace in *Theorie Analytique des probabilities* - 1812
- Gauss furthered least mean squares - 1821
- Term regression coined by Francis Galton, and furthered ideas through regression to the mean in paper *Regression Towards Mediocrity in Hereditary Stature* - 1886
- Karl Pearson and Udny Yule furthered regression for a general statistical context - 1900s
- Markov chains invented by Andrey Markov - 1913
- IBM made punch cards that use regression - 1920s
- Model of electronic neurons made - 1943
- Book points out neurons strengthen, Hebb's Rule - 1949
- Game theory created - 1945
- Nathanial Rochester attempted to make neural network, failed and little is known - 1950s
- Turning's Learning machine - 1950
- Turning Test - 1950
- Marvin Minsky and Dean Edmonds build first neural network SNARC - 1951
- Machines play checkers Arthur Samuel, form of computer learning - 1952
- Logical theorist program by Allen Newell is the first program to use AI - 1956
- Perceptron invented by Frank Rosenblatt, paper written about it - 1957/58
- MADALINE by Bernard Widrow and Marcian Hoff of Stanford apply the first neural network to a real world problem - 1959
- Nathaniel Rochester at IBM develops geometry proving algorithm which is faster than humans - 1959
- Rosenblatt's *Principles of Neurodynamics* notes neural network discoveries - 1962
- Widrow and Hoff develop a learning procedure, needs more research to verify
- Tic Tac Toe Machine Donald Michie uses Reinforcement learning - 1963
	- Alexey Ivakhnenko and V. G. Lapa do multilayer networks - 1965
- ALPAC report noted AI research was bad leading to AI winter in USA - 1966
- Nearest Neighbors algorithm created - 1967
- Alexey Ivakhnenko and V. G. Lapa do a generalized feed forward algorithm in *cybernetics forcecasting techniques*- 1967
- Limitations of neural networks by Minsky and Seymour Papert leads to slowdown of AI progress - 1969
- AI Winter - 1970 to 1980
- Automatic Differentiation (Backpropogation) invented by Sepp Linnainmaa - 1970
- Ivakhnenko publishes paper *polynomial theory of complex systems* - 1971
- Kohonen and Anderson create similar neural networks separately - 1972
- Lighthill report furthers this winter within the UK - 1973
- First multilayered neural network developed - 1975
- Werbos develops develops backpropagation algorithm
- Neocognitron invented by Kunihiko Fukushima - 1979
- Stanford cart - 1979
- People realised multiple layers in a neural network increase accuracy - 1980s
- Explanation Based Learning invented Gerald Dejong - 1981
- Recurrent Neural Network Invented by John Hopfield - 1982
- Reilley and Cooper use *hybrid network* - 1982
- Joint US-Japan conference on Cooperative/Competitive Neural Networks leads to greater advances - 1982
- Werbos first uses back propogation on neural networks - 1982
- Net talk simulates a baby learning to speak - 1985
- Neural networks in computing established by American institute of Physics - 1985
- LeCun finds back propogation - 1985
- Back propagation used by Rumelhart, Geoff Hinton, and Ronald J. Williams on hidden layers of a neural network - 1986
- Paul Smolensky invents Restricted Boltzmann networks - 1986
- International conference at IEEE on neural networks - 1987
- Time Delay Network form of CNN introduced by Alex Waibel - 1987
- Radial Basis network created by Broomhead and Lowe in a paper - 1988
- Christopher Watkins develops Q-learning - 1989
- Yann LeCun used standard backpropagation algorithm, this used a CNN I think - 1989
- Commercialization of Machine Learning by Axcelis Inc in Evolver
- Machines Learning becomes data-driven approach rather than knowledge - 1990s
- AI Winter 2, caused by SVMs and other popular methods of AI - 1990s
- Machines play backgammon - 1992
- Max pooling invented which would later be used in 2010 however just a general technique invented - 1992
- Schmidhuber adopted a multilevel hierarchy of networks pretrained by unsupervised learning and finetuned by backpropogation, this used to counter vanishing gradient problem - 1992
- Random forest algorithm invented by Tin Kam Ho - 1995
- Support vector machines invented by Corinna Cortes and Vladimir Vapnik - 1995
- Deep Blue beats Kasparov - 1997
- LSTM developed by Sepp Hochreiter and Jurgen Schmidhuber - 1997
- MNIST database created by Yann Lecun - 1998
- Yoshua Mengio is relevant in deep learing - 2000s
- Nvidia creates first true GPU - 1999
- Torch library released - 2002
- Geoffrey Hinton coins deep learning - 2006
- First GPU implementation of a CNN by K. Chellapilla - 2006
- Hinton popularises restricted Boltzmann network - 2006
- Nvidia supports neural networks - 2009
- Dan Ciresan used backpropagation on CNN ran om GPU - 2010
- Generative Adversarial network invented by Ian Goodfellow - 2014
The rest of the 21st century sees history based on ML achievements

Von Neumann Architecture being developed may have lead to AI winter. People also hyped up neural networks too much. DARPA and other organisations needed more concrete results from AI research if they wanted to fund.
Rosenblatt differentiate perceptron style neural networks and Minsky's symbol manipulation neural nets. Rosenblatt's work is more about the

Optimisation techniques need to be explored, such as gradient descent and others.

Look at neural network funding

## Website Sources
- https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html - Gives good history with a focus on Stanford developments. Concludes with the view that neural networks should be considered an alternative to von Neumann computer architecture, especially in consideration of analog machines.
- https://en.wikipedia.org/wiki/Timeline_of_machine_learning - Decent timeline.
- https://www.dataversity.net/a-brief-history-of-neural-networks/ - Stripped down timeline which focuses on teaching concepts.
- https://www.analyticsvidhya.com/blog/2021/04/the-history-of-neural-networks/ - Talks about important neural network developments, ignores some of the neural network types for some reason.
- R. C. Eberhart and R. W. Dobbins, "Early neural network development history: the age of Camelot," in IEEE Engineering in Medicine and Biology Magazine, vol. 9, no. 3, pp. 15-18, Sept. 1990, doi: 10.1109/51.59207 - Splits AI history into the age of Camelot (Perceptron to Minsky's AI limits paper 1979), dark ages until Hopefield's paper on neural networks published in 1982 which starts the renaissance, and then the Neo-connectionism which starts with the Parrel Distributed Processing by Rumelhart and McClelland om 1986. Brings up William Jame's ideas on neurology.
	- Notes that the McCulloch-Pitt neuron isn't as relevant with many mathematical operations being discarded in modern networks. These include on and off neurons, threshold to excite neurons being a certain amount of other neurons, and many more details. Furthermore the paper is hard to read.
	- Hebb noted the weight is kept in the synapse, postulated that a learning weight is proportional to activation of neurons, noted weights are symmetric, theorised weights change as learning occurs.
- https://priceonomics.com/the-discovery-of-statistical-regression/ - Good look towards the 1800s history of regression, debates Legendre vs Gauss. Ignores regression's later uses.
- https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414 - Fucks up some facts wrong dates on perceptron book, SVMs invented in 1980s? GPU architecture is similar to a neural network, need to research
- https://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec - Concepts of backpropagation established since 1960's by Paul Werbos. This wasn't noticed until 1985 after a paper by Parker?
- https://arxiv.org/pdf/2109.01517.pdf - Excellent general history of AI, make sure to reuse.
- https://www.youtube.com/watch?v=QW_srPO-LrI - Minsky Interview on limitations of neural networks.
- https://people.idsia.ch/~juergen/who-invented-backpropagation.html#HIN - Good article to understand backpropagation history.

## Key Papers
- McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. _The bulletin of mathematical biophysics_, _5_(4), 115–133.
- Hebb, D. O. (1964). _The organization of behavior: A neuropsychological theory_. New York: John Wiley.
- Von Neumann, J., & Morgenstern, O. (2007). Theory of games and economic behavior. Princeton University Press.
- Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. _Psychological Review, 65_(6), 386–408.
- Rosenblatt, Frank.  (1962).  _Principles of neurodynamics; perceptrons and the theory of brain mechanisms_.  Washington :  Spartan Books
- Minsky, M. (1988). _Perceptrons : an introduction to computational geometry_ (Expanded ed.). Cambridge, Mass.: Cambridge, Mass. : MIT Press.
- Bernard Widrow (1960). “Adaptive "Adaline" Neuron Using Chemical "memistors” Number Technical Report 1553-2. Stanford Electron. Labs. Stanford, CA.
- Linnainmaa, Seppo. (1976). Taylor expansion of the accumulated rounding error. _BIT_, _16_(2), 146–160. https://doi.org/10.1007/BF01931367 - 1970 Automatic Diff Algorithms

## References by Subject

### SNARC
https://historyof.ai/snarc/ - Recreation of SNARC, through a guy who asked Minsky's daughter about the project.
https://www.webofstories.com/play/marvin.minsky/136 - Minsky Interviews, points out there are photos that exist, and shows part of the machine. Wikipedia pretty much based itself off this interview.