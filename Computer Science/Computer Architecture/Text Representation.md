As all computers are built up of numeric components in order to simulate text we have standards which convert given numbers to a character format. The most common representation of this is the **ASCII** (American Standard Code) standard. This was the default way of expressing characters within the English language on computers during the 20th century. However, due to the constraints of the ASCII standard's limit of 8 bit characters a more global standard would need to be adopted as to allow for foreign characters.

Built off of ASCII **Unicode** become the universal standard as it allowed over a million possible characters. This standard is further divided into **UTF-8** and **UTF-16** depending on the desired character limit. UTF-8 is more commonly used.